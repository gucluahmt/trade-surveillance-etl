# ðŸ§© Trade Data ETL Process with Validation Rules 


---

## ðŸ§­ Project Overview

This project demonstrates a realistic **ETL (Extract, Transform, Load)** pipeline for trade datasets.  
It simulates how raw trading data (from an OMS feed or CSV source) is:

1. **Extracted** â€” read from raw input files.  
2. **Transformed** â€” normalized into a canonical schema.  
3. **Enriched** â€” combined with reference data (product & client master).  
4. **Validated** â€” checked against field-level, schema-level, and business rules.  
5. **Loaded** â€” saved into curated folders for analytics or reporting use.

The pipeline ensures that all data passing into the next stage is clean, accurate, and aligned with defined data standards.

---

## ðŸ§¾ Business Impact

| Impact Area | Description |
|--------------|-------------|
| **Data Reliability** | Guarantees that downstream systems (analytics, reporting, or compliance) receive only validated and consistent trade data. |
| **Operational Efficiency** | Reduces manual data reviews by detecting and isolating data issues early in the ETL pipeline. |
| **Auditability** | Each run is fully traceable â€” timestamps, validation rules, and output files are versioned for audit readiness. |
| **Decision Confidence** | Business stakeholders can make faster, data-driven decisions with confidence in the accuracy of underlying data. |
| **Cost Optimization** | Preventing data quality issues early lowers reprocessing and reconciliation costs. |

> **Clean data = Confident decisions.**  
> This validation framework ensures that every record entering the business ecosystem is accurate, complete, and compliant with defined rules.



---


### Tech Stack
- **Language:** Python 3.11  
- **Libraries:** pandas, logging, pathlib, json  
- **Structure:** Modular ETL with separate ingestion, enrichment, and validation layers  

---

## ðŸ§© Pipeline Stages

| Stage | Description | Output |
|--------|--------------|---------|
| **Ingestion** | Reads raw CSV files and converts them into canonical schema | Normalized DataFrame |
| **Enrichment** | Joins product & client master data and derives new fields | Enriched DataFrame |
| **Validation** | Runs data-quality and business rules | Curated + Exception outputs |
| **Outcome** | Stores results: curated (passed), exceptions (failed), metrics (summary) | Files in `20_outcome/` |

---

### Validation Rule Set 

| Rule ID            | Description                                                       | Severity  |
|--------------------|-------------------------------------------------------------------|-----------|
| R001_MANDATORY     | Mandatory fields must not be null (`trade_id`, `order_id`, etc.) | CRITICAL  |
| R002_ENUMS         | Enum checks for `side`, `instrument_type`, `currency`            | HIGH      |
| R003_POSITIVE      | `quantity` and `price` must be > 0                               | HIGH      |
| R004_ID_FORMAT     | ISIN / CUSIP format verification (regex-based)                   | MEDIUM    |
| R005_TS_SANITY     | `trade_ts` must be on/after `trade_date`                         | MEDIUM    |
| R006_NOTIONAL      | `notional â‰ˆ quantity Ã— price` within tolerance                   | LOW       |
| R007_DUPLICATES    | Duplicate `trade_id` or (`order_id`,`trade_ts`,`quantity`,`price`)| CRITICAL |


---

## ðŸ“Š Example Run Summary

```bash
INFO | ingestion | Starting ingestion...
INFO | enrichment | Enrichment completed.
INFO | validation | Validation complete. Passed=8460 Failed=1540 Breaches=1566
=== Validation Summary ===
{
  "input_rows": 10000,
  "passed_rows": 8460,
  "failed_rows": 1540,
  "pass_rate_pct": 84.6,
  "run_ts_utc": "2025-10-21T16:46:17Z"
}

PROJECT STRUCTURE
â”œâ”€â”€ 00_input/
â”‚   â”œâ”€â”€ inbound/              # raw trade feeds
â”‚   â””â”€â”€ reference/            # product_master.csv, client_master.csv
â”œâ”€â”€ 10_process/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ core/             # config, utils, logger
â”‚       â”œâ”€â”€ p01_ingestion/    # schema normalization
â”‚       â”œâ”€â”€ p02_enrichment/   # join reference data
â”‚       â”œâ”€â”€ p03_validation/   # rule engine + validator
â”‚       â””â”€â”€ pipeline.py       # orchestrator 
â”œâ”€â”€ 20_outcome/
â”‚   â”œâ”€â”€ curated/              # clean validated data
â”‚   â”œâ”€â”€ exceptions/           # failed records
â”‚   â””â”€â”€ metrics/              # validation summary
â””â”€â”€ README.md
